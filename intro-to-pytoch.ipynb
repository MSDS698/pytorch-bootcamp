{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-Pytorch\" data-toc-modified-id=\"Intro-to-Pytorch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro to Pytorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pytorch-tensors\" data-toc-modified-id=\"Pytorch-tensors-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Pytorch tensors</a></span></li><li><span><a href=\"#Pytorch-Autograd\" data-toc-modified-id=\"Pytorch-Autograd-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pytorch Autograd</a></span></li><li><span><a href=\"#torch.nn-module\" data-toc-modified-id=\"torch.nn-module-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>torch.nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch consists of 4 main packages:\n",
    "* torch: a general purpose array library similar to Numpy that can do computations on GPU\n",
    "* torch.autograd: a package for automatically obtaining gradients\n",
    "* torch.nn: a neural net library with common layers and cost functions\n",
    "* torch.optim: an optimization package with common optimization algorithms like SGD, Adam, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors\n",
    "Like Numpy tensors but can utilize GPUs to accelerate its numerical computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a tensor\n",
    "data = torch.tensor([\n",
    "                     [0, 1],    \n",
    "                     [2, 3],\n",
    "                     [4, 5]\n",
    "                    ])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a tensor as data type = torch.float32 / torch.int\n",
    "data = torch.tensor([\n",
    "                     [0 , 1],    \n",
    "                     [2, 3],\n",
    "                     [4, 5]\n",
    "                    ],dtype=torch.float32)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0178, -0.7224,  0.0757, -0.8119, -1.3555],\n",
       "        [ 0.5471,  0.3311,  1.4192,  0.0902,  0.4487],\n",
       "        [-2.4475,  1.2038, -0.9069, -0.2007,  1.8423]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random tensor\n",
    "N = 3\n",
    "x = torch.randn(N, 5).type(torch.FloatTensor)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0606, -1.3085,  1.4893],\n",
       "         [ 2.0778,  0.1684,  0.5583],\n",
       "         [ 0.7862,  0.5500, -0.4482]],\n",
       "\n",
       "        [[ 0.2579,  1.0363,  0.3196],\n",
       "         [ 1.1917,  0.7270,  0.7504],\n",
       "         [-0.4242,  0.0099,  0.6363]],\n",
       "\n",
       "        [[-0.0620, -1.2823,  0.9859],\n",
       "         [-0.3995,  0.5984,  0.9968],\n",
       "         [ 1.3148,  0.2202, -0.6200]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3x3x3 Tensor\n",
    "s = [3,3,3]\n",
    "b = torch.randn(s)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from a list\n",
    "l = [[1,2],[3,4]]\n",
    "l\n",
    "tensor = torch.tensor(l)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a tensor of all zeros / ones\n",
    "zeros = torch.zeros(2, 5).type(torch.LongTensor)  \n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range from [1,10]\n",
    "rr = torch.arange(1,10)\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a torch.tensor tensor([[1, 0, 5]])\n",
      "This is a np.ndarray [[1 0 5]]\n"
     ]
    }
   ],
   "source": [
    "# numpy.ndarray --> torch.Tensor:\n",
    "arr = np.array([[1, 0, 5]])\n",
    "data = torch.tensor(arr)\n",
    "print(\"This is a torch.tensor\", data)\n",
    "\n",
    "# torch.Tensor --> numpy.ndarray:\n",
    "new_arr = data.numpy()\n",
    "print(\"This is a np.ndarray\", new_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape is: torch.Size([2, 2])\n",
      "Tensor type is: torch.int64\n",
      "Tensor device is: cpu\n"
     ]
    }
   ],
   "source": [
    "print('Tensor shape is:',tensor.shape)\n",
    "print('Tensor type is:',tensor.dtype)\n",
    "print('Tensor device is:',tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is: torch.Size([12])\n",
      "The contents are: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "\n",
      "After reshaping, the shape is: torch.Size([4, 3])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "rr = torch.arange(1, 13)\n",
    "print(\"The shape is:\", rr.shape)\n",
    "print(\"The contents are:\", rr)\n",
    "print()\n",
    "rr = rr.view(4, 3)\n",
    "print(\"After reshaping, the shape is:\", rr.shape)\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping of tensors using .view()\n",
    "rr.view(1,-1) #-1 makes torch infer the second dim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx is tensor([[1, 2],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "yy is tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "The product is tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n",
      "The other product is tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n"
     ]
    }
   ],
   "source": [
    "xx = torch.tensor([[1, 2], [2, 3], [4, 5]])      # (3, 2)\n",
    "yy = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4) then xx * yy = (3, 4)\n",
    "\n",
    "print(\"xx is\", xx)\n",
    "print(\"yy is\", yy)\n",
    "print(\"The product is\", xx.matmul(yy))\n",
    "print(\"The other product is\", xx @ yy) # +, -, *, @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10.],\n",
      "        [11., 12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19., 20.],\n",
      "        [21., 22., 23., 24., 25.]])\n",
      "Taking the sum over columns:\n",
      "tensor([55., 60., 65., 70., 75.])\n",
      "Taking thep sum over rows:\n",
      "tensor([ 15.,  40.,  65.,  90., 115.])\n",
      "Taking the stdev over rows:\n",
      "tensor([1.5811, 1.5811, 1.5811, 1.5811, 1.5811])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(1, 26, dtype=torch.float32).reshape(5,5)\n",
    "print(data)\n",
    "\n",
    "# We can perform operations like *sum* over each row...\n",
    "print(\"Taking the sum over columns:\")\n",
    "print(data.sum(dim=0))\n",
    "\n",
    "# or over each column.\n",
    "print(\"Taking thep sum over rows:\")\n",
    "print(data.sum(dim=1))\n",
    "\n",
    "# Other operations are available:\n",
    "print(\"Taking the stdev over rows:\")\n",
    "print(data.std(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Write code that creates a `torch.tensor` with the following contents:\n",
    "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
    "\n",
    "Now compute the average of each row (`.mean()`) and each column.\n",
    "\n",
    "What's the shape of the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 15.,  40.,  65.,  90., 115.])\n",
      "torch.Size([5])\n",
      "tensor([[ 15.,  40.,  65.,  90., 115.]])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "## Be careful with dimension\n",
    "test = data.sum(dim=1)\n",
    "print(test)\n",
    "print(test.shape)\n",
    "print(test.unsqueeze(dim=0))\n",
    "print(test.unsqueeze(dim=0).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,13).view(3,2,2)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  6],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]      # first row equivalent to x[0,:]\n",
    "x[:,0]    # for every row (dim = 0), print first row\n",
    "x[0,1,1]  # 0 (first matrix), 1 (second row of first matrix), 1 (element 2)\n",
    "x[:,0,0]  # top left element of each row->(array)\n",
    "\n",
    "# Let's access the 0th and 1st elements, each twice (list of arrays)\n",
    "i = torch.tensor([0, 0, 1, 1])\n",
    "print(x[i])\n",
    "\n",
    "# Let's access the 0th elements of the 1st and 2nd elements\n",
    "i = torch.tensor([1, 2])\n",
    "j = torch.tensor([0])\n",
    "x[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n",
      "tensor([1, 2, 3])\n",
      "tensor([ 1,  4,  7, 10, 13])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[ 1,  2],\n",
      "        [ 4,  5],\n",
      "        [ 7,  8],\n",
      "        [10, 11],\n",
      "        [13, 14]])\n",
      "tensor([[1, 2],\n",
      "        [4, 5],\n",
      "        [7, 8]])\n",
      "tensor(3)\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 7,  8,  9],\n",
      "        [13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "matr = torch.arange(1, 16).view(5, 3)\n",
    "print(matr)\n",
    "print(matr[0])         # elements from first row (dim = 0) eq to x[0,:]\n",
    "print(matr[:,0])       # elements from first column (dim = 0)\n",
    "print(matr[0:3])       # rows from 0 to 3\n",
    "print(matr[:,0:2])     # for every row print columns 0 and 1\n",
    "print(matr[0:3,0:2])   # for rows 0,1,2 print columns 0 and 1\n",
    "print(matr[0][2])      # element row=0 , column=2\n",
    "print(matr[[0, 2, 4]]) # rows 0,2,4 [list[0,2,4]] mask!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "The autograd package in PyTorch provides classes and functions implementing automatic differentiation of arbitrary scalar valued function. For example, the gradient of the error with respect to all parameters.\n",
    "\n",
    "In order for this to happen we need to declare our parameters as Tensors with the requires_grad=True keyword.\n",
    "We can call the backward() method to ask PyTorch to calculate the gradients, which are then stored in the grad attribute.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the gradient of y with respect to x\n",
    "\n",
    "y = x * x * 3     # 3x^2\n",
    "y.backward()  # computes the grad of y with respect to x\n",
    "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "# backprop again\n",
    "\n",
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "print(x.grad)    # After each training iteration (aka. coeficient update) Run \"zero_grad()\" to avoid gradient accumulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn module\n",
    "A neural net library with common layers and cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3419,  0.3712, -0.1048,  0.4108, -0.0980],\n",
       "         [ 0.0902, -0.2177,  0.2626,  0.3942, -0.3281],\n",
       "         [ 0.3887,  0.0837,  0.3304,  0.0606,  0.2156]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0631,  0.3448,  0.0661], requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear transformation of a Nx5 matrix into a Nx3 matrix, where N can be anything \n",
    "# (number of observations)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "D = 5 # number of input featutes\n",
    "M = 3 # neurons in the first hidden layer\n",
    "linear_map = nn.Linear(D, M)\n",
    "\n",
    "# parameters are initialized randomly\n",
    "[p for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.3301,  0.1802],\n",
       "                      [-0.3258, -0.0829]])),\n",
       "             ('0.bias', tensor([-0.2872,  0.4691])),\n",
       "             ('2.weight', tensor([[-0.5582, -0.3260]])),\n",
       "             ('2.bias', tensor([-0.1997]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define a block\n",
    "\n",
    "model_seq = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 2),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(2, 1)\n",
    ")\n",
    "print(model_seq)\n",
    "model_seq.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - n_features\n",
    "        - hiden layer\n",
    "        - output layer\n",
    "        \"\"\"\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(3,2)\n",
    "        self.output = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.3471,  0.0545, -0.5702],\n",
       "                      [ 0.5214, -0.4904,  0.4457]])),\n",
       "             ('hidden.bias', tensor([ 0.0961, -0.1875])),\n",
       "             ('output.weight', tensor([[0.4370, 0.1102]])),\n",
       "             ('output.bias', tensor([0.5713]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNetwork()\n",
    "model\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3471,  0.0545, -0.5702],\n",
       "        [ 0.5214, -0.4904,  0.4457]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.hidden.weight.data\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3471,  0.0545, -0.5702],\n",
       "         [ 0.5214, -0.4904,  0.4457]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0961, -0.1875], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.4370, 0.1102]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5713], requires_grad=True)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = list(model.parameters())\n",
    "parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3471,  0.0545, -0.5702],\n",
       "         [ 0.5214, -0.4904,  0.4457]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.0961, -0.1875]),\n",
       " Parameter containing:\n",
       " tensor([[0.4370, 0.1102]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5713], requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.hidden.parameters():\n",
    "    param.requires_grad = False\n",
    "    print(param.requires_grad)\n",
    "    \n",
    "parameters = list(model.parameters())\n",
    "parameters     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "optim.step() #gradient descent. The optimizer adjusts each parameter by its gradient stored in .grad.\n",
    "             # weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5547, 0.3423, 0.6343]])\n",
      "tensor([[1.]])\n",
      "loss=0.1639 y_pred=0.60\n",
      "loss=0.1073 y_pred=0.67\n",
      "loss=0.0703 y_pred=0.73\n",
      "loss=0.0460 y_pred=0.79\n",
      "loss=0.0302 y_pred=0.83\n",
      "loss=0.0197 y_pred=0.86\n",
      "loss=0.0129 y_pred=0.89\n",
      "loss=0.0085 y_pred=0.91\n",
      "loss=0.0055 y_pred=0.93\n",
      "loss=0.0036 y_pred=0.94\n"
     ]
    }
   ],
   "source": [
    "# Data 1 training example (X) 3 features, 1 label (Y)\n",
    "x = torch.rand(1,3)\n",
    "print(x)\n",
    "\n",
    "y = torch.ones(1,1)\n",
    "print(y)\n",
    "\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "for t in range(100):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "    if t % 10 == 0: print(\"loss={:.4f}\".format(loss.item()), 'y_pred={:.2f}'.format(y_pred.item()))\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "* https://pytorch.org/docs/stable/index.html\n",
    "* http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "* https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "116px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
